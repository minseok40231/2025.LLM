{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1r8ZAgDFaGbGfhGQgqElz-LmYOYYAM4Zu",
      "authorship_tag": "ABX9TyPgZgliuIN8BqvSt9DVTu5F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minseok40231/2025.LLM/blob/main/202021004%EA%B9%80%EB%AF%BC%EC%84%9D_OpenAI_API_%EB%B0%B0%ED%8F%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
        "\n",
        "def transcribe_audio(file_path):\n",
        "    with open(file_path, \"rb\") as audio_file:\n",
        "        response = client.audio.transcriptions.create(\n",
        "            model=\"whisper-1\",\n",
        "            file=audio_file,\n",
        "            response_format=\"json\",\n",
        "            temperature=0.1\n",
        "        )\n",
        "    return response.text\n",
        "\n",
        "file_path = \"Dracula.mp3\"\n",
        "\n",
        "transcription = transcribe_audio(file_path)\n",
        "print(\"Transcription Response:\")\n",
        "print(transcription)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4kGhve-rEq4",
        "outputId": "38a19d60-773d-4212-a5d8-0d372e006939"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription Response:\n",
            "Now that we've found where the enemy's lurking, nothing can stand in our way. Since we are facing the forces of darkness, we must be the cold light of day. We are the lanterns that burn in the lighthouse. The candles in the crypt. We are the light. Let there be light. This is a war and we must be the victors. There's too much to lose if we fail. We'll cross the seas like a band of crusaders, searching for some precious grail. We are the embers that glow in the winter. The diamonds in the mine. Let's take our torches and pray God will show us a sign. Deep in the darkness night, when there's a spark of hope, we must be voice of light. He's in the darkness, bright as the dazzling stars in a different sky. And in our cruel last hour, when hope is gone, we'll raise our heads and we'll turn the odds. When the great battle commences, surely the light will prevail. We will break down his defenses. He will fall. And the sun will rise. Deep in the darkness night, when there's a spark of hope, we must be voice of light. He's in the darkness, bright as the dazzling stars in a different sky. And in our cruel last hour, when hope is gone, we'll raise our heads and we'll turn the odds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
        "\n",
        "upload_response = client.files.create(\n",
        "    file=open(\"king-style-chat.jsonl\", \"rb\"),\n",
        "    purpose=\"fine-tune\"\n",
        ")\n",
        "print(\"Upload Response:\")\n",
        "print(upload_response)\n",
        "\n",
        "list_response = client.files.list()\n",
        "print(\"List Response:\")\n",
        "print(list_response)\n",
        "\n",
        "file_id = upload_response.id\n",
        "retrieve_response = client.files.retrieve(file_id)\n",
        "print(\"Retrieve Response:\")\n",
        "print(retrieve_response)\n",
        "\n",
        "delete_response = client.files.delete(file_id)\n",
        "print(\"Delete Response:\")\n",
        "print(delete_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHYS5KTbqx8w",
        "outputId": "81cdf13e-619a-4b75-814a-193bad6df179"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload Response:\n",
            "FileObject(id='file-VgzVVCN1XV9AsXg9Sy5Lim', bytes=57615, created_at=1764135481, filename='king-style-chat.jsonl', object='file', purpose='fine-tune', status='processed', expires_at=None, status_details=None)\n",
            "List Response:\n",
            "SyncCursorPage[FileObject](data=[FileObject(id='file-VgzVVCN1XV9AsXg9Sy5Lim', bytes=57615, created_at=1764135481, filename='king-style-chat.jsonl', object='file', purpose='fine-tune', status='processed', expires_at=None, status_details=None), FileObject(id='file-5S7zxERfe9GgPv8jygNhcD', bytes=2168, created_at=1764135044, filename='step_metrics.csv', object='file', purpose='fine-tune-results', status='processed', expires_at=None, status_details=None), FileObject(id='file-9NygmqEnvGs9QxpBSF34NR', bytes=3128, created_at=1764133913, filename='mydata.jsonl', object='file', purpose='fine-tune', status='processed', expires_at=None, status_details=None), FileObject(id='file-FYCgA7jSQkCJffR8qk6wpt', bytes=1948, created_at=1764048341, filename='step_metrics.csv', object='file', purpose='fine-tune-results', status='processed', expires_at=None, status_details=None), FileObject(id='file-5BXDkMaHmVf5zf2QMn6PK8', bytes=3128, created_at=1764047138, filename='mydata.jsonl', object='file', purpose='fine-tune', status='processed', expires_at=None, status_details=None), FileObject(id='file-PqtoR3W14gBTW1okr5d7zZ', bytes=3128, created_at=1764047110, filename='mydata.jsonl', object='file', purpose='fine-tune', status='processed', expires_at=None, status_details=None), FileObject(id='file-9VgoqdQdmQwRC9kMNuZezq', bytes=3128, created_at=1764045908, filename='mydata.jsonl', object='file', purpose='fine-tune', status='processed', expires_at=None, status_details=None)], has_more=False, object='list', first_id='file-VgzVVCN1XV9AsXg9Sy5Lim', last_id='file-9VgoqdQdmQwRC9kMNuZezq')\n",
            "Retrieve Response:\n",
            "FileObject(id='file-VgzVVCN1XV9AsXg9Sy5Lim', bytes=57615, created_at=1764135481, filename='king-style-chat.jsonl', object='file', purpose='fine-tune', status='processed', expires_at=None, status_details=None)\n",
            "Delete Response:\n",
            "FileDeleted(id='file-VgzVVCN1XV9AsXg9Sy5Lim', deleted=True, object='file')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
        "\n",
        "def generate_code(prompt, model=\"gpt-4o-mini\", max_tokens=1000):\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful coding assistant. Write the code requested.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=0\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "prompt = \"Write a C code that computes Fibonacci number using memoization.\"\n",
        "\n",
        "generated_code = generate_code(prompt)\n",
        "\n",
        "if generated_code:\n",
        "    print(\"Generated Code:\\n\")\n",
        "    print(generated_code)\n",
        "else:\n",
        "    print(\"Failed to generate code.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_T1XP21qgZW",
        "outputId": "e7153f95-f63f-4d8d-c7e6-f3c764291c54"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Code:\n",
            "\n",
            "Certainly! Below is a C program that computes Fibonacci numbers using memoization. This approach stores previously computed Fibonacci numbers in an array to avoid redundant calculations, significantly improving performance for larger values of `n`.\n",
            "\n",
            "```c\n",
            "#include <stdio.h>\n",
            "\n",
            "#define MAX 100 // Define the maximum size for memoization array\n",
            "\n",
            "// Array to store Fibonacci numbers\n",
            "long long memo[MAX];\n",
            "\n",
            "// Function to compute Fibonacci number using memoization\n",
            "long long fibonacci(int n) {\n",
            "    // Base cases\n",
            "    if (n <= 0) return 0;\n",
            "    if (n == 1) return 1;\n",
            "\n",
            "    // Check if the value is already computed\n",
            "    if (memo[n] != -1) {\n",
            "        return memo[n];\n",
            "    }\n",
            "\n",
            "    // Compute the Fibonacci number and store it in the memo array\n",
            "    memo[n] = fibonacci(n - 1) + fibonacci(n - 2);\n",
            "    return memo[n];\n",
            "}\n",
            "\n",
            "int main() {\n",
            "    // Initialize the memoization array with -1\n",
            "    for (int i = 0; i < MAX; i++) {\n",
            "        memo[i] = -1;\n",
            "    }\n",
            "\n",
            "    int n;\n",
            "    printf(\"Enter a positive integer to compute Fibonacci: \");\n",
            "    scanf(\"%d\", &n);\n",
            "\n",
            "    if (n < 0 || n >= MAX) {\n",
            "        printf(\"Please enter a number between 0 and %d\\n\", MAX - 1);\n",
            "        return 1;\n",
            "    }\n",
            "\n",
            "    long long result = fibonacci(n);\n",
            "    printf(\"Fibonacci(%d) = %lld\\n\", n, result);\n",
            "\n",
            "    return 0;\n",
            "}\n",
            "```\n",
            "\n",
            "### Explanation:\n",
            "1. **Memoization Array**: We define an array `memo` to store Fibonacci numbers that have already been computed. The size of the array is defined by `MAX`.\n",
            "2. **Base Cases**: The function `fibonacci` checks for the base cases where `n` is 0 or 1.\n",
            "3. **Memoization Check**: Before computing the Fibonacci number, the function checks if it has already been computed (i.e., if `memo[n]` is not -1).\n",
            "4. **Recursive Calculation**: If the value is not computed, it calculates it recursively and stores the result in the `memo` array.\n",
            "5. **Input Handling**: The program prompts the user for input and checks if the input is within the valid range.\n",
            "\n",
            "### Usage:\n",
            "Compile the program using a C compiler (e.g., `gcc`) and run the executable. You can input a positive integer to get the corresponding Fibonacci number.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from google.colab import userdata\n",
        "\n",
        "client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
        "\n",
        "def generate_image(prompt):\n",
        "    response = client.images.generate(\n",
        "        model=\"dall-e-3\",\n",
        "        prompt=prompt,\n",
        "        n=1,\n",
        "        size=\"1024x1024\"\n",
        "    )\n",
        "    return response.data[0].url\n",
        "\n",
        "def save_image(image_url, filename):\n",
        "    response = requests.get(image_url)\n",
        "    image = Image.open(BytesIO(response.content))\n",
        "    image.save(filename)\n",
        "\n",
        "prompt = \"A futuristic cityscape at sunset\"\n",
        "\n",
        "image_url = generate_image(prompt)\n",
        "print(f\"Image URL: {image_url}\")\n",
        "\n",
        "save_image(image_url, \"generated_image.png\")\n",
        "print(\"Image saved as generated_image.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXbOF5T0qKVZ",
        "outputId": "a37a38f5-0eca-4937-ad05-663d144b067a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image URL: https://oaidalleapiprodscus.blob.core.windows.net/private/org-R8Gqu2ZFkMnzcSxRRQWVEJXZ/user-EEQOXE5xtgNssldv2J8MIpjF/img-cbM2sIRhIrHurqgM1va5lNjc.png?st=2025-11-26T04%3A35%3A32Z&se=2025-11-26T06%3A35%3A32Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=b2c0e1c0-cf97-4e19-8986-8073905d5723&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-11-25T14%3A40%3A40Z&ske=2025-11-26T14%3A40%3A40Z&sks=b&skv=2024-08-04&sig=WshvCqTaW0l5NVvgWbVDWQGDHPgoCfbhbfuEJV%2BldkY%3D\n",
            "Image saved as generated_image.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
        "\n",
        "def moderate_text(input_text):\n",
        "    response = client.moderations.create(\n",
        "        input=input_text,\n",
        "        model=\"omni-moderation-latest\"\n",
        "    )\n",
        "    return response.results[0]\n",
        "\n",
        "input_texts = [\n",
        "    \"I want to harm myself.\",\n",
        "    \"You are an amazing person!\",\n",
        "    \"Let's meet at 8 PM.\",\n",
        "    \"I hate you and I want to hurt you.\"\n",
        "]\n",
        "\n",
        "for text in input_texts:\n",
        "    moderation_result = moderate_text(text)\n",
        "    print(f\"Input: {text}\")\n",
        "    print(f\"Moderation Result: {moderation_result}\")\n",
        "    print(\"-\" * 40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EuikZOJpkCy",
        "outputId": "7b42911d-f403-442b-cbef-1fc0a62d0397"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: I want to harm myself.\n",
            "Moderation Result: Moderation(categories=Categories(harassment=False, harassment_threatening=False, hate=False, hate_threatening=False, illicit=False, illicit_violent=False, self_harm=True, self_harm_instructions=False, self_harm_intent=True, sexual=False, sexual_minors=False, violence=True, violence_graphic=False, harassment/threatening=False, hate/threatening=False, illicit/violent=False, self-harm/intent=True, self-harm/instructions=False, self-harm=True, sexual/minors=False, violence/graphic=False), category_applied_input_types=CategoryAppliedInputTypes(harassment=['text'], harassment_threatening=['text'], hate=['text'], hate_threatening=['text'], illicit=['text'], illicit_violent=['text'], self_harm=['text'], self_harm_instructions=['text'], self_harm_intent=['text'], sexual=['text'], sexual_minors=['text'], violence=['text'], violence_graphic=['text'], harassment/threatening=['text'], hate/threatening=['text'], illicit/violent=['text'], self-harm/intent=['text'], self-harm/instructions=['text'], self-harm=['text'], sexual/minors=['text'], violence/graphic=['text']), category_scores=CategoryScores(harassment=0.0005689574642911562, harassment_threatening=0.0007696328798416084, hate=1.7952796934677738e-05, hate_threatening=8.888084683809127e-06, illicit=0.0052831760048374175, illicit_violent=2.4156629828672456e-05, self_harm=0.9746147566202028, self_harm_instructions=0.00028151729221049604, self_harm_intent=0.9885491614739444, sexual=8.559006367452268e-05, sexual_minors=6.205049602300744e-06, violence=0.4023404152908457, violence_graphic=0.001610160050623429, harassment/threatening=0.0007696328798416084, hate/threatening=8.888084683809127e-06, illicit/violent=2.4156629828672456e-05, self-harm/intent=0.9885491614739444, self-harm/instructions=0.00028151729221049604, self-harm=0.9746147566202028, sexual/minors=6.205049602300744e-06, violence/graphic=0.001610160050623429), flagged=True)\n",
            "----------------------------------------\n",
            "Input: You are an amazing person!\n",
            "Moderation Result: Moderation(categories=Categories(harassment=False, harassment_threatening=False, hate=False, hate_threatening=False, illicit=False, illicit_violent=False, self_harm=False, self_harm_instructions=False, self_harm_intent=False, sexual=False, sexual_minors=False, violence=False, violence_graphic=False, harassment/threatening=False, hate/threatening=False, illicit/violent=False, self-harm/intent=False, self-harm/instructions=False, self-harm=False, sexual/minors=False, violence/graphic=False), category_applied_input_types=CategoryAppliedInputTypes(harassment=['text'], harassment_threatening=['text'], hate=['text'], hate_threatening=['text'], illicit=['text'], illicit_violent=['text'], self_harm=['text'], self_harm_instructions=['text'], self_harm_intent=['text'], sexual=['text'], sexual_minors=['text'], violence=['text'], violence_graphic=['text'], harassment/threatening=['text'], hate/threatening=['text'], illicit/violent=['text'], self-harm/intent=['text'], self-harm/instructions=['text'], self-harm=['text'], sexual/minors=['text'], violence/graphic=['text']), category_scores=CategoryScores(harassment=0.0006602641499937108, harassment_threatening=1.0391067562761452e-05, hate=6.10885510164857e-06, hate_threatening=1.5057017254045334e-07, illicit=6.40200641038395e-06, illicit_violent=4.198630823683514e-06, self_harm=1.1235328063870752e-05, self_harm_instructions=1.1300808333434096e-06, self_harm_intent=5.144221374220898e-06, sexual=1.15919343186331e-05, sexual_minors=1.3007128466476034e-06, violence=0.0005323933551511255, violence_graphic=4.832563818725537e-06, harassment/threatening=1.0391067562761452e-05, hate/threatening=1.5057017254045334e-07, illicit/violent=4.198630823683514e-06, self-harm/intent=5.144221374220898e-06, self-harm/instructions=1.1300808333434096e-06, self-harm=1.1235328063870752e-05, sexual/minors=1.3007128466476034e-06, violence/graphic=4.832563818725537e-06), flagged=False)\n",
            "----------------------------------------\n",
            "Input: Let's meet at 8 PM.\n",
            "Moderation Result: Moderation(categories=Categories(harassment=False, harassment_threatening=False, hate=False, hate_threatening=False, illicit=False, illicit_violent=False, self_harm=False, self_harm_instructions=False, self_harm_intent=False, sexual=False, sexual_minors=False, violence=False, violence_graphic=False, harassment/threatening=False, hate/threatening=False, illicit/violent=False, self-harm/intent=False, self-harm/instructions=False, self-harm=False, sexual/minors=False, violence/graphic=False), category_applied_input_types=CategoryAppliedInputTypes(harassment=['text'], harassment_threatening=['text'], hate=['text'], hate_threatening=['text'], illicit=['text'], illicit_violent=['text'], self_harm=['text'], self_harm_instructions=['text'], self_harm_intent=['text'], sexual=['text'], sexual_minors=['text'], violence=['text'], violence_graphic=['text'], harassment/threatening=['text'], hate/threatening=['text'], illicit/violent=['text'], self-harm/intent=['text'], self-harm/instructions=['text'], self-harm=['text'], sexual/minors=['text'], violence/graphic=['text']), category_scores=CategoryScores(harassment=3.459916031782155e-05, harassment_threatening=1.7130819343483194e-05, hate=8.349627818261147e-06, hate_threatening=9.223470110117277e-07, illicit=3.353501304664781e-05, illicit_violent=2.4923252458203565e-05, self_harm=9.610241549947397e-06, self_harm_instructions=3.535625074174432e-06, self_harm_intent=0.000211865052952336, sexual=4.955359475635505e-05, sexual_minors=6.502816785764437e-06, violence=0.0005600758955536546, violence_graphic=4.832563818725537e-06, harassment/threatening=1.7130819343483194e-05, hate/threatening=9.223470110117277e-07, illicit/violent=2.4923252458203565e-05, self-harm/intent=0.000211865052952336, self-harm/instructions=3.535625074174432e-06, self-harm=9.610241549947397e-06, sexual/minors=6.502816785764437e-06, violence/graphic=4.832563818725537e-06), flagged=False)\n",
            "----------------------------------------\n",
            "Input: I hate you and I want to hurt you.\n",
            "Moderation Result: Moderation(categories=Categories(harassment=True, harassment_threatening=True, hate=False, hate_threatening=False, illicit=False, illicit_violent=False, self_harm=False, self_harm_instructions=False, self_harm_intent=False, sexual=False, sexual_minors=False, violence=True, violence_graphic=False, harassment/threatening=True, hate/threatening=False, illicit/violent=False, self-harm/intent=False, self-harm/instructions=False, self-harm=False, sexual/minors=False, violence/graphic=False), category_applied_input_types=CategoryAppliedInputTypes(harassment=['text'], harassment_threatening=['text'], hate=['text'], hate_threatening=['text'], illicit=['text'], illicit_violent=['text'], self_harm=['text'], self_harm_instructions=['text'], self_harm_intent=['text'], sexual=['text'], sexual_minors=['text'], violence=['text'], violence_graphic=['text'], harassment/threatening=['text'], hate/threatening=['text'], illicit/violent=['text'], self-harm/intent=['text'], self-harm/instructions=['text'], self-harm=['text'], sexual/minors=['text'], violence/graphic=['text']), category_scores=CategoryScores(harassment=0.8057718838706006, harassment_threatening=0.5509624992902102, hate=0.0011864146960012469, hate_threatening=5.527786369235996e-05, illicit=0.020043798361113124, illicit_violent=0.0020194050134869097, self_harm=0.0005302147677935735, self_harm_instructions=0.0002145314506303538, self_harm_intent=0.0002763292760766304, sexual=0.0007067749478525128, sexual_minors=7.722191834067996e-06, violence=0.8507924307636365, violence_graphic=4.373344035182828e-05, harassment/threatening=0.5509624992902102, hate/threatening=5.527786369235996e-05, illicit/violent=0.0020194050134869097, self-harm/intent=0.0002763292760766304, self-harm/instructions=0.0002145314506303538, self-harm=0.0005302147677935735, sexual/minors=7.722191834067996e-06, violence/graphic=4.373344035182828e-05), flagged=True)\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
        "\n",
        "def edit_text(input_text, instruction):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that edits text.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Please edit the following text: '{input_text}'. Instruction: {instruction}\"}\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "input_text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "instruction = \"Change 'fox' to 'cat' and change the tense to past.\"\n",
        "\n",
        "edited_text = edit_text(input_text, instruction)\n",
        "print(f\"Edited text: {edited_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TZ_AettnR3H",
        "outputId": "4d87e4e5-d359-47c6-a8b1-8bbf96ee020b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Edited text: The quick brown cat jumped over the lazy dog.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from openai import OpenAI\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
        "file_response = client.files.create(\n",
        "    file=open(\"mydata.jsonl\", \"rb\"),\n",
        "    purpose=\"fine-tune\"\n",
        ")\n",
        "file_id = file_response.id\n",
        "print(f\"Uploaded file ID: {file_id}\")\n",
        "\n",
        "job_response = client.fine_tuning.jobs.create(\n",
        "    training_file=file_id,\n",
        "    model=\"gpt-4o-mini-2024-07-18\"\n",
        ")\n",
        "job_id = job_response.id\n",
        "print(f\"Fine-tune job ID: {job_id}\")\n",
        "\n",
        "while True:\n",
        "    job_status = client.fine_tuning.jobs.retrieve(job_id)\n",
        "    status = job_status.status\n",
        "    print(f\"Fine-tune job status: {status}\")\n",
        "\n",
        "    if status in [\"succeeded\", \"failed\", \"cancelled\"]:\n",
        "        break\n",
        "\n",
        "    time.sleep(60)\n",
        "\n",
        "if status == \"succeeded\":\n",
        "    fine_tuned_model = job_status.fine_tuned_model\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=fine_tuned_model,\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": \"Translate the following English text to French: 'Good night'\"}\n",
        "        ],\n",
        "        max_tokens=50\n",
        "    )\n",
        "    print(f\"Fine-tuned model output: {response.choices[0].message.content}\")\n",
        "else:\n",
        "    print(\"Fine-tuning job failed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQ-ocFrukjwk",
        "outputId": "bead9257-86ed-455c-95e5-272927d21aac"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded file ID: file-9NygmqEnvGs9QxpBSF34NR\n",
            "Fine-tune job ID: ftjob-wRSAbEANsPZDxGvyG6E8hS7V\n",
            "Fine-tune job status: validating_files\n",
            "Fine-tune job status: validating_files\n",
            "Fine-tune job status: running\n",
            "Fine-tune job status: running\n",
            "Fine-tune job status: running\n",
            "Fine-tune job status: running\n",
            "Fine-tune job status: running\n",
            "Fine-tune job status: running\n",
            "Fine-tune job status: running\n",
            "Fine-tune job status: running\n",
            "Fine-tune job status: running\n",
            "Fine-tune job status: running\n",
            "Fine-tune job status: running\n",
            "Fine-tune job status: running\n",
            "Fine-tune job status: running\n",
            "Fine-tune job status: running\n",
            "Fine-tune job status: running\n",
            "Fine-tune job status: running\n",
            "Fine-tune job status: running\n",
            "Fine-tune job status: succeeded\n",
            "Fine-tuned model output: Bonne nuit.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nS45eX-6ia_L",
        "outputId": "a4116685-585d-4237-a355-b8532f338411"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "red: [9.326533472631127e-06, -0.02476814016699791, -0.002384250983595848, -0.028791459277272224, -0.021199282258749008]...\n",
            "1536\n",
            "blue: [0.005474964156746864, -0.007486246060580015, 0.005678507499396801, -0.03110414557158947, -0.01965053379535675]...\n",
            "1536\n",
            "yellow: [0.007661858107894659, -0.024910997599363327, 0.004491548519581556, -0.02860249951481819, -0.01958620548248291]...\n",
            "1536\n",
            "green: [0.01546180434525013, -0.010975971817970276, 0.025183379650115967, -0.02092933841049671, -0.005648194346576929]...\n",
            "1536\n",
            "violet: [-0.006727131083607674, -0.018318135291337967, 0.0036361967213451862, -0.00567674869671464, -0.021194979548454285]...\n",
            "1536\n",
            "cyan: [0.021550633013248444, -0.014010688289999962, 0.008289773017168045, -0.02929886430501938, -0.016149088740348816]...\n",
            "1536\n",
            "black: [-0.015103082172572613, -0.031215764582157135, 0.00877943355590105, -0.03691864386200905, -0.01613996922969818]...\n",
            "1536\n",
            "white: [0.006292110309004784, -0.02457117661833763, 0.0002028137823799625, -0.014848269522190094, -0.0052642603404819965]...\n",
            "1536\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
        "\n",
        "def get_embeddings(texts):\n",
        "    response = client.embeddings.create(\n",
        "        input=texts,\n",
        "        model=\"text-embedding-ada-002\"\n",
        "    )\n",
        "    return [item.embedding for item in response.data]\n",
        "\n",
        "color_words = [\"red\", \"blue\", \"yellow\", \"green\", \"violet\", \"cyan\", \"black\", \"white\"]\n",
        "\n",
        "color_embeddings = get_embeddings(color_words)\n",
        "\n",
        "for word, embedding in zip(color_words, color_embeddings):\n",
        "    print(f\"{word}: {embedding[:5]}...\")\n",
        "    print(len(embedding))"
      ]
    }
  ]
}